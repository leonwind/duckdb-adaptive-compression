\documentclass[aspectratio=43]{beamer}
\usepackage{caption}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{pgf}

\lstdefinestyle{C++}{language=C++,
                showstringspaces=false,
                basicstyle=\ttfamily,
                morekeywords={size_t,size_in_bytes,bit_compress,extractMinFromVector,width},
                keywordstyle=\color{blue}\ttfamily,
                stringstyle=\color{red}\ttfamily,
                commentstyle=\color{green}\ttfamily,
                morecomment=[l][\color{magenta}]{\#}
}
\lstdefinestyle{shell}{basicstyle=\ttfamily}
\lstdefinestyle{SQL}{language=SQL,basicstyle=\ttfamily, keywordstyle=\color{blue}\ttfamily}


\title{(Not yet Adaptive) Compression of In-Memory Databases}
\subtitle{Database Implementation Lab Course}
\author{Leon Windheuser}
\date{March 14, 2023}


\begin{document}

\frame{\titlepage}


\begin{frame}
    \frametitle{Project Introduction}
    \textbf{Goal: We want to compress the transient part of DuckDB}
    \pause

    \begin{itemize}
        \item Open Source SQL OLAP RDBMS in-process developed in Amsterdam research center CWI (SQLite for OLAP)
        \url{https://github.com/duckdb/duckdb}
        \item Columnar storage format
        \item Vectorized execution engine
        \item Has already lots of different compression possibilities for persistent data on disk 
    \end{itemize}

    \vspace{1cm}
    \pause
    \centering
    \textbf{How do we compress the transient data while having efficient lookups without decompressing everything?}
\end{frame}


\begin{frame}
    \frametitle{Background: Succinct Data Structures}
    
    \begin{itemize}
        \item Data structures that use space close to the theoretic lower bound but allow efficient query operations (in place without needing to decompress)
        \item Exists e.g. {\only<+>{(bit) vectors}\only<+->{\textbf{(bit) vectors}}}, trees, planar graphs, ...
    \end{itemize}
\end{frame}

% Example in db: Primary key and Enums (4 bytes but only 17 categories)
\begin{frame}
    \frametitle{Succinct Integer Vector}
    \centering
    Space requirement for integer $x$ is $\ell = \lceil \log_2(x) \rceil$ bits

    \includegraphics[width=\framewidth]{figures/excalidraw/bit-int-vector.png}
    \pause

    Encode integers with the minimal length of the max integer $3 = \lceil \log_2(7) \rceil$ \\
    \vspace{0.5cm} 
    \includegraphics[width=0.75\framewidth]{figures/excalidraw/bit-compressed-int-vector.png}

    \vspace{0.5cm}
    \textbf{We already reduce memory by 25\%}
\end{frame}


\begin{frame}
    \frametitle{SDSL: Succinct Data Structure Library}

    \begin{itemize}
        \item C++11 library for succinct data structures
        \item Open Source \url{https://github.com/simongog/sdsl-lite}
        \item Contains a variety of succinct data structures. 
        We use \textbf{Integer Vector} to compress integer columns.
    \end{itemize}
\end{frame}


\begin{frame}[fragile]
    \frametitle{SDSL: Integer Vectors}

\begin{lstlisting}[style=C++,escapechar=!]
sdsl::int_vector<32> v(10000);
for (size_t i = 0; i < 10000; i++) v[i] = i;
cout << "Width: " << v.width() << ", size: " 
     << sdsl::size_in_bytes(v) << endl;
!\colorbox{yellow}{sdsl::util::bit\_compress(v);}!
cout << "Width: " << v.width() << ", size: " 
     << sdsl::size_in_bytes(v) << endl;
\end{lstlisting}

\pause

\begin{lstlisting}[style=shell]
Width: 32, size: 40008
Width: 14, size: 17513
\end{lstlisting}

\pause

\begin{center}
    Reduces memory by 56.2\% ($\approx$ 22.5 KB)
\end{center}
\end{frame}


\begin{frame}[fragile]
    \frametitle{Delta Compression of \ttfamily{sdsl::int\_vector}}

\begin{lstlisting}[style=C++]
sdsl::int_vector<32> v(10000);
for (size_t i = 0; i < 10000; i++) 
    v[i] = i + 10.000.000;
cout << "Width: " << v.width() << ", size: " 
     << sdsl::size_in_bytes(v) << endl;
sdsl::util::bit_compress(v);
cout << "Width: " << v.width() << ", size: " 
     << sdsl::size_in_bytes(v) << endl;
\end{lstlisting}

\begin{lstlisting}[style=shell]
Width: 32, size: 40008
Width: 24, size: 30008
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]
    \frametitle{Delta Compression of \ttfamily{sdsl::int\_vector}}
\begin{lstlisting}[style=C++,escapechar=!]
sdsl::int_vector<32> v(10000);
for (size_t i = 0; i < 10000; i++) 
    v[i] = i + 10.000.000;
cout << "Width: " << v.width() << ", size: " 
     << sdsl::size_in_bytes(v) << endl;
!\colorbox{yellow}{extractMinFromVector(v);}!
sdsl::util::bit_compress(v);
cout << "Width: " << v.width() << ", size: " 
     << sdsl::size_in_bytes(v) << endl;
\end{lstlisting}

\begin{lstlisting}[style=shell]
Width: 32, size: 40008
Width: 14, size: 17513
\end{lstlisting}

\pause
In DuckDB, we know the minimum of the vector directly without searching (column statistics)
\end{frame}


\begin{frame}
    \frametitle{DuckDB Storage Architecture Bird's Eye View (100 meters)}
    \includegraphics[width=\framewidth]{figures/excalidraw/duckdb-high-level-storage-arch.png}
\end{frame}


\begin{frame}
    \frametitle{DuckDB Storage Architecture Bird's Eye View (10 meters)}
    \includegraphics[width=\framewidth]{figures/excalidraw/duckdb-column-segment-look.png}
\end{frame}


\begin{frame}[fragile]
    \frametitle{Evaluation: Sequential Insert and Total Scan}
Scanning 1M rows.

\begin{lstlisting}[style=SQL]
SELECT * FROM t1;
\end{lstlisting}
\scalebox{0.9}{
    \input{../experiments/plots/pgf/Total_Memory_of_SequentialInsert.pgf}
}
\end{frame}


\begin{frame}[fragile]
    \frametitle{Evaluation: Sequential Insert and Total Scan}
Scanning 1M rows.

\begin{lstlisting}[style=SQL]
SELECT * FROM t1;
\end{lstlisting}
\scalebox{0.9}{
    \input{../experiments/plots/pgf/Runtime_of_SequentialInsert.pgf}
}
\end{frame}


\begin{frame}[fragile]
    \frametitle{Evaluation: Zipf Selection}
10000 selections with Zipf distribution of 1M total rows.

\begin{lstlisting}[style=SQL]
SELECT i FROM t1 
WHERE i == {ZIPF_DISTRIBUTED_NUMBER};
\end{lstlisting}

\scalebox{0.9}{
    \input{../experiments/plots/pgf/Total_Memory_of_ZipfDistribution.pgf}
}
\end{frame}


\begin{frame}[fragile]
    \frametitle{Evaluation: Zipf Selection}
10000 selections with Zipf distribution of 1M total rows.

\begin{lstlisting}[style=SQL]
SELECT i FROM t1 
WHERE i == {ZIPF_DISTRIBUTED_NUMBER};
\end{lstlisting}
   

\scalebox{0.9}{  
\input{../experiments/plots/pgf/Runtime_of_ZipfDistribution.pgf}
 }
\end{frame}

\begin{frame}[fragile]
    \frametitle{Evaluation: Zipf Out-Of-Memory (Limit 1GB)}
\begin{lstlisting}[style=SQL]
SELECT i FROM t1 
WHERE i == {ZIPF_DISTRIBUTED_NUMBER};
\end{lstlisting}

\scalebox{0.9}{
\input{../experiments/plots/pgf/Total_Memory_of_ZipfScanOOM.pgf}
}
\end{frame}


\begin{frame}[fragile]
    \frametitle{Evaluation: Zipf Out-Of-Memory (Limit 1GB)}
\begin{lstlisting}[style=SQL]
SELECT i FROM t1 
WHERE i == {ZIPF_DISTRIBUTED_NUMBER};
\end{lstlisting}

\scalebox{0.9}{
\input{../experiments/plots/pgf/Runtime_of_ZipfScanOOM.pgf}
}
\end{frame}


\begin{frame}
    \frametitle{Conclusion}

    \begin{itemize}
        \item For OLAPish queries: It is not (yet?) worth it (large overhead due to copying).
        \item For OLTP transactions:  It might be worth it. Reduces memory by $\approx 40\%$ but increases runtime by $\approx 35\%$.
        \item Great benefit if succinct representation fits in memory vs. spilling to disk.
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Future Work and Discussion}

    \begin{enumerate}

    \item Copying and shifting data is most time consuming ($\approx$ 40\%) since execution engine expects a 
    flat ``normal'' vector.
    \begin{itemize}
        \item Non-succinct passes its data pointer that we need to decompress and copy the data.
        \item Unnecessary since we still support random access and operations needed for the execution engine.
        \item Non-succinct data pointer used everywhere in the execution engine ($>300$ appearances). 
        \textbf{Rewrite necessary?}
    \end{itemize}

    \vspace{0.3cm} 

    \item Adaptive compression for rarely accessed segments. 
    Zipf Distribution accesses 4/50 segments over $70\%$ of the time.
    \begin{itemize}
        \item How to track access statistics over time for segments?
        \item What if the access statistics change after a greater period of time?
    \end{itemize}
\end{enumerate}

\end{frame}


\end{document}
